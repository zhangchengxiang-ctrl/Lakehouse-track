# Lakehouse-track：埋点全链路（Paimon Hive Metastore）
# 使用方式：docker compose up -d --build
# 最小资源模式：各服务已配置内存限制，合计约 8GB
#
# 镜像说明：postgres:16 redis:7 minio/minio flink:1.20.3-scala_2.12
#          starrocks/fe-ubuntu:3.5.12 starrocks/cn-ubuntu:3.5.12（存算分离，JDK17+）
#          apache/streampark:2.1.6 apache/kafka:3.7.0
#          nginx:latest timberio/vector:0.53.0-alpine（采集端拆分为两镜像，共享 nginx_logs）
# 简化：StreamPark 元数据复用 PostgreSQL，已移除 MySQL
#
# 前置准备：
#   1. bash scripts/lakehouse.sh install
# MinIO 的 bucket 与 staging 前缀会由 minio-init 自动创建
#
# StreamPark 启动：docker compose --profile streampark up -d streampark（需先拷出 Flink 到 flink/dist/flink）
#
services:
  # ========== 埋点链路 (Lakehouse-track) ==========
  postgres:
    image: postgres:16
    command: ["postgres", "-c", "wal_level=logical"]  # PG CDC 必需，首次部署生效
    mem_limit: 512m
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: paimon
      POSTGRES_PASSWORD: paimon123
      POSTGRES_DB: paimon_db
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
      - ./postgres/init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U paimon -d paimon_db"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - lakehouse-net

  redis:
    image: redis:7
    mem_limit: 256m
    ports: 
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      retries: 5
    networks:
      - lakehouse-net

  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    mem_limit: 512m
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./data/minio:/data
    networks:
      - lakehouse-net

  minio-init:
    image: minio/mc
    depends_on:
      minio:
        condition: service_started
    entrypoint: >
      /bin/sh -c "
      for i in $(seq 1 30); do
        /usr/bin/mc alias set myminio http://minio:9000 minioadmin minioadmin && break;
        sleep 2;
      done;
      /usr/bin/mc mb myminio/paimon-lake || true;
      printf \"\" | /usr/bin/mc pipe myminio/paimon-lake/staging/.keep || true;
      printf \"\" | /usr/bin/mc pipe myminio/paimon-lake/paimon_data/.keep || true;
      exit 0;
      "
    networks:
      - lakehouse-net


  # 采集端：Nginx 写日志 -> 共享目录 -> Vector 读并写入 MinIO
  nginx:
    image: nginx:latest
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/sdk_debug:/etc/nginx/sdk_debug:ro
      - ./data/nginx_logs:/var/log/nginx
    networks:
      - lakehouse-net

  vector:
    image: timberio/vector:0.53.0-alpine
    command: ["--config", "/etc/vector/vector.yaml"]
    mem_limit: 256m
    volumes:
      - ./data/nginx_logs:/var/log/nginx:ro
      - ./vector/vector.yaml:/etc/vector/vector.yaml:ro
      - ./vector/geoip:/etc/vector/geoip:ro
    depends_on:
      minio:
        condition: service_started
    restart: on-failure
    networks:
      - lakehouse-net

  # ========== Flink ==========
  # 使用自定义镜像（flink/Dockerfile）避免 bind mount 覆盖 lib 导致丢失 flink-dist
  flink-jobmanager:
    build:
      context: .
      dockerfile: flink/Dockerfile
    container_name: flink-jobmanager
    hostname: flink-jobmanager
    command: jobmanager
    mem_limit: 2g
    volumes:
      - ./flink/flink.sql:/opt/flink/flink.sql:ro
    environment:
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - ENABLE_S3_PATH_STYLE_ACCESS=true
      - "FLINK_PROPERTIES=jobmanager.rpc.address: flink-jobmanager"
    ports:
      - "8081:8081"
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_started
      hive-metastore:
        condition: service_started
    networks:
      - lakehouse-net

  flink-taskmanager:
    build:
      context: .
      dockerfile: flink/Dockerfile
    container_name: flink-taskmanager
    hostname: flink-taskmanager
    command: taskmanager
    mem_limit: 2g
    environment:
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - ENABLE_S3_PATH_STYLE_ACCESS=true
      - "FLINK_PROPERTIES=jobmanager.rpc.address: flink-jobmanager"
    depends_on:
      - flink-jobmanager
    networks:
      - lakehouse-net

  # ========== Hive Metastore (Standalone) ==========
  hive-metastore:
    image: apache/hive:3.1.3
    container_name: hive-metastore
    user: "0:0"
    restart: on-failure
    entrypoint: ["/bin/sh", "-c"]
    command: "/opt/hive/bin/hms-entrypoint.sh"
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - HIVE_CONF_hive_metastore_warehouse_dir=s3a://paimon-lake/paimon_data/
      - HIVE_CONF_hive_metastore_disallow_incompatible_col_type_changes=false
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - SERVICE_NAME=metastore
      - DB_DRIVER=postgres
      - >-
        SERVICE_OPTS=-Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore
        -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver
        -Djavax.jdo.option.ConnectionUserName=paimon
        -Djavax.jdo.option.ConnectionPassword=paimon123
    ports:
      - "9083:9083"
    volumes:
      - ./flink/lib/postgresql-42.7.3.jar:/opt/hive/lib/postgresql-42.7.3.jar
      - ./hive/hms-lib/hadoop-aws-3.1.0.jar:/opt/hive/lib/hadoop-aws-3.1.0.jar
      - ./hive/hms-lib/aws-java-sdk-bundle-1.11.271.jar:/opt/hive/lib/aws-java-sdk-bundle-1.11.271.jar
      - ./hive/hive-site.xml:/opt/hive/conf/hive-site.xml:ro
      - ./hive/hms-entrypoint.sh:/opt/hive/bin/hms-entrypoint.sh:ro
    networks:
      - lakehouse-net

  # streampark:
  #   image: apache/streampark:2.1.6
  #   container_name: streampark
  #   profiles:
  #     - streampark
  #   ports:
  #     - "10000:10000"
  #   volumes:
  #     - ./flink/lib:/opt/streampark/lib
  #     - ./flink/dist/flink:/opt/flink
  #   environment:
  #     - DATASOURCE_DIALECT=pgsql
  #     - DATASOURCE_URL=jdbc:postgresql://postgres:5432/streampark?stringtype=unspecified
  #     - DATASOURCE_USERNAME=paimon
  #     - DATASOURCE_PASSWORD=paimon123
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #   networks:
  #     - lakehouse-net

  # ========== 消息队列与分析层（可选，当前埋点链路未使用） ==========
  # kafka:
  #   image: apache/kafka:3.7.0
  #   profiles:
  #     - kafka
  #   container_name: kafka
  #   environment:
  #     KAFKA_NODE_ID: 1
  #     KAFKA_PROCESS_ROLES: broker,controller
  #     KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:9093"
  #     KAFKA_LISTENERS: "PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093"
  #     KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:9092"
  #     KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
  #     KAFKA_LOG_DIRS: "/var/lib/kafka/data"
  #   volumes:
  #     - ./data/kafka:/var/lib/kafka/data
  #   ports:
  #     - "9092:9092"
  #   networks:
  #     - lakehouse-net

  # ========== StarRocks 存算分离（FE + CN，数据存 MinIO） ==========
  starrocks-fe:
    image: starrocks/fe-ubuntu:3.5.12
    container_name: starrocks-fe
    hostname: starrocks-fe
    mem_limit: 4g
    environment:
      - HOST_TYPE=FQDN
      - JAVA_TOOL_OPTIONS=--add-opens=java.base/java.util=ALL-UNNAMED
    command:
      - /bin/bash
      - -c
      - |
        # 1) 依赖注入：使用专用子目录隔离
        mkdir -p /opt/starrocks/fe/lib/paimon-ext
        ln -sf /opt/starrocks-extra-jars/*.jar /opt/starrocks/fe/lib/paimon-ext/
        
        # 2) 合并共享配置
        if ! grep -q "BEGIN LAKEHOUSE_TRACK_SHARED_CONF" /opt/starrocks/fe/conf/fe.conf 2>/dev/null; then
          {
            echo ""
            echo "# BEGIN LAKEHOUSE_TRACK_SHARED_CONF"
            cat /opt/starrocks-fe-shared.conf
            echo "# END LAKEHOUSE_TRACK_SHARED_CONF"
          } >> /opt/starrocks/fe/conf/fe.conf
        fi
        /opt/starrocks/fe/bin/start_fe.sh
    ports:
      - "8030:8030"
      - "9020:9020"
      - "9030:9030"
    volumes:
      - ./starrocks/config/fe-shared.conf:/opt/starrocks-fe-shared.conf:ro
      - ./starrocks/jars:/opt/starrocks-extra-jars:ro
      - ./data/starrocks/fe:/opt/starrocks/fe/meta
    healthcheck:
      test: ["CMD-SHELL", "mysql -uroot -h127.0.0.1 -P9030 -e 'SELECT 1'"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 120s
    depends_on:
      minio:
        condition: service_started
    networks:
      - lakehouse-net

  starrocks-cn:
    image: starrocks/cn-ubuntu:3.5.12
    container_name: starrocks-cn
    hostname: starrocks-cn
    mem_limit: 4g
    environment:
      - HOST_TYPE=FQDN
      - JAVA_TOOL_OPTIONS=--add-opens=java.base/java.util=ALL-UNNAMED
    command:
      - /bin/bash
      - -c
      - |
        # 1) 依赖注入
        mkdir -p /opt/starrocks/cn/lib/paimon-ext
        ln -sf /opt/starrocks-extra-jars/*.jar /opt/starrocks/cn/lib/paimon-ext/
        
        # 2) 幂等添加 CN 节点
        sleep 15
        mysql -h starrocks-fe -P9030 -uroot -e "SHOW COMPUTE NODES" | grep -q "starrocks-cn" || \
        mysql -h starrocks-fe -P9030 -uroot -e "ALTER SYSTEM ADD COMPUTE NODE \"starrocks-cn:9050\";"
        
        /opt/starrocks/cn/bin/start_cn.sh
    ports:
      - "8040:8040"
    volumes:
      - ./starrocks/jars:/opt/starrocks-extra-jars:ro
      - ./data/starrocks/cn/storage:/opt/starrocks/cn/storage
      - ./data/starrocks/cn/datacache:/opt/starrocks/cn/datacache
    depends_on:
      starrocks-fe:
        condition: service_healthy
      minio:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "mysql -uroot -hstarrocks-fe -P9030 -e 'SHOW COMPUTE NODES' | grep 'starrocks-cn' | grep -q '1'"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 180s
    restart: on-failure
    networks:
      - lakehouse-net

networks:
  lakehouse-net:
    name: lakehouse-net
    driver: bridge

volumes:
  # 使用本地文件夹挂载，不再需要命名的 volume
  {}
  # postgres_data:
  # minio_data:
  # kafka_data:
  # nginx_logs:
  # starrocks_fe_data:
  # starrocks_cn_data:
